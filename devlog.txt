NUTSHELL DEVLOG

=== 2025-10-28: Initial implementation ===

Built basic working version of nutshell - CLI tool for summarizing research papers with Claude API.

Current state:
- Core functionality working: PDF in, markdown summary out
- Using Claude 3.5 Haiku (claude-3-5-haiku-20241022) - supports PDF input, cheap for dev work
- Tested with one paper (LLM Agents in Interaction), produced good summary
- Modular code structure: separate functions for PDF loading, prompt creation, API calls, file I/O
- Git repo initialized, 2 commits so far

Key decisions made:
- PDF analysis (full document) vs text extraction: chose PDF because Haiku 3.5 supports it
  Means figures/tables/formatting preserved. Can revisit later if needed.
- Model selection is parameterized, easy to swap for prompt engineering phase
- Kept initial version simple: no research focus param yet, just core loop working

What's working:
- API integration solid
- PDF handling clean
- Output quality looks good (but only one test case so far)

What's untested:
- Different paper types (math-heavy, very long, short letters, multi-column layouts)
- Edge cases (malformed PDFs, rate limits, network issues)
- Prompt quality across diverse content
- Token usage for large papers (might hit max_tokens limit)

Next steps:
- Test with diverse papers to validate robustness
- Refine prompt based on learnings
- Then add features (research focus param, etc)

Notes:
- API key stored in .env (gitignored)
- Sample paper in project dir for quick testing
- Prompt is unmodified from original - probably needs refinement after more testing
- max_tokens set to 4096, might need adjustment for comprehensive summaries

=== 2025-10-28: Infrastructure testing ===

Ran comprehensive tests across 7 diverse papers:
- 6-page bio paper with diagrams
- 9-page AI paper
- Math-heavy two-column physics paper
- Social sciences experimental stats paper
- 41-page paper with code snippets
- 41-page figure/table-heavy paper
- 90-page key field paper

Results: ALL TESTS PASSED
- No errors, timeouts, or truncation issues
- PDF parsing robust across different formats
- Consistent output size (65-106 lines, ~2.6-3.8K)
- max_tokens=4096 is adequate

Infrastructure is solid and ready for production use.

Key insight: Summary length stays relatively consistent regardless of input length.
This is actually ideal - even 90-page papers compress down to ~4K summaries.

Minor issue: Filenames with special characters (smart quotes) need wildcards,
but this is a shell escaping issue, not a tool problem.

Next phase:
- Infrastructure validated, ready to move to quality improvements
- Switch to better model (Sonnet 4.0/4.5) for production
- Consider prompt engineering once quality becomes the focus
- Could add features: research focus, URL download, batch processing

=== 2025-10-28: Switch to Sonnet 4.5 ===

Changed default model from Haiku 3.5 to Sonnet 4.5 for quality work.

Decision: Keep it simple, just change the default. Model selection already
available via -m flag, no need for config file complexity yet.

Initial testing shows significant quality improvement:
- Haiku 3.5: ~80 line summaries
- Sonnet 4.5: ~254 line summaries (3x more comprehensive)
- Much more detailed methodology, specific quotes, statistical values
- Better structure and organization

Minor note: Sonnet 4.5 includes scratchpad in output (prompt artifact).
May want to refine prompt to exclude scratchpad from final output.

Ready to evaluate quality across diverse papers and refine as needed.

=== 2025-10-28: Prompt management infrastructure + scratchpad removal ===

Set up prompt management system:
- Created prompts/ directory with versioned prompt files
- prompts/v1_baseline.txt: original prompt with scratchpad instruction
- prompts/v2_no_scratchpad.txt: removed scratchpad section
- prompts/changelog.txt: tracks changes between versions
- Modified code to load prompts from files via -p flag
- Default now uses v2_no_scratchpad.txt

Research finding: Claude 4.x models think internally by default. The explicit
scratchpad instruction in our prompt was causing it to output planning process.
Solution: simply remove the scratchpad instruction. Model still plans, just
doesn't output it.

Testing confirms scratchpad removed successfully. Output now starts directly
with reference document content, no planning artifacts.

Prompt versioning infrastructure ready for iterative quality improvements.

=== 2025-11-19: Convert to installable package with subcommands ===

Restructured project into proper Python package for system-wide installation.

Package structure:
- nutshell_pkg/ - main package directory
  - __init__.py - package metadata
  - cli.py - command-line interface with subcommand support
  - core.py - core summarization functionality
- setup.py - installation configuration
- MANIFEST.in - ensures Prompts/ directory included in distribution

Command structure:
- Old: python nutshell.py paper.pdf
- New: nutshell summarize paper.pdf

Benefits:
- Can run from any directory after installation
- Cleaner command interface with subcommands
- Extensible for future features (other subcommands can be added)
- Proper package structure for distribution

Installation: pip install -e . (editable mode for development)

Tested successfully: command works from any location, accesses prompts correctly.

=== 2025-11-19: Add config file support for API key ===

Problem: API key only available when exported in current directory, not system-wide.

Solution: Implemented config file support at ~/.config/nutshell/config

Implementation:
- Added load_api_key() function in core.py
- Checks in order: environment variable, then config file
- Config file format: ANTHROPIC_API_KEY=<key>
- Follows XDG Base Directory specification
- Not typically version controlled (safer than dotfiles)

Updated summarize_paper() to use load_api_key() when creating Anthropic client.

Benefits:
- Works from any directory on the system
- Doesn't pollute dotfiles that might be shared/version controlled
- Falls back to environment variable if config file not present

Tested: Successfully ran nutshell from home directory with config file API key.

=== 2025-11-19: Add transcribe feature ===

Created new transcribe subcommand for full paper transcriptions.

Feature: nutshell transcribe
- Complete verbatim transcription of paper text
- Converts figures/charts to detailed text descriptions + data tables
- Converts all tables to markdown format
- Higher token limit (16384) for longer transcriptions
- Adds HTML disclaimer comment at top of output

Implementation:
- Created Prompts/transcribe_v1.txt (adapted from draft)
- Added transcribe_paper() and save_transcription() to core.py
- Added cmd_transcribe() and transcribe parser to cli.py
- Updated documentation in README

Key differences from summarize:
- Preserves ALL content verbatim vs. summary
- 16K token limit vs. 4K for summaries
- Output suffix: _transcription.md vs. _summary.md
- Includes disclaimer HTML comment

Testing: Successfully transcribed sample paper (418 lines, 36K).

Use case: Create context-optimized versions of papers for AI conversations,
preserving all content while converting visual elements to text.

=== 2025-11-19: Add token usage and cost reporting ===

Added automatic token/cost reporting at end of summarize and transcribe operations.

Implementation:
- Updated summarize_paper() and transcribe_paper() to return (text, usage) tuple
- Added calculate_cost() function with current model pricing
- Updated CLI handlers to unpack usage and display stats
- Format: "Tokens: X,XXX in, X,XXX out" and "Cost: $X.XXXX"

Pricing (per million tokens):
- Haiku 3.5: $0.80 input, $4.00 output
- Sonnet 4.5: $3.00 input, $15.00 output

Testing note: For development/testing, use Haiku by default with -m flag.

Tested: chicken.pdf with Haiku (8,219 in, 500 out, $0.0086)

=== 2025-11-19: Add PDF URL support with caching (dev branch) ===

Added ability to process PDFs from URLs (e.g., arXiv papers).

Implementation:
- Added download_pdf_from_url() to download and cache PDFs
- Cache location: ~/.cache/nutshell/pdfs/
- Uses MD5 hash of URL as cache filename
- Automatically reuses cached PDFs for same URL
- Added extract_arxiv_id() to extract IDs from arXiv URLs for output naming
- Updated CLI to accept URLs in addition to file paths
- Output files use arXiv ID when available (e.g., 2402.02896_summary.md)

Benefits:
- No need to manually download papers
- Cached downloads save bandwidth and time
- Clean separation of cache from working directories
- Works with arXiv and any direct PDF URL

Future enhancement: Extract paper metadata (author, title) for better output naming

Testing: Successfully processed https://isotropic.org/papers/chicken.pdf
Cache works correctly on subsequent runs.

=== 2025-11-19: Add model shortname system (dev branch) ===

Implemented flexible model shortname system (hybrid approach).

Shortcuts available:
- Canonical: haiku, sonnet, opus (stable, curated defaults)
- Version-specific: haiku-3.5, sonnet-4.5, opus-3
- Latest: haiku-latest, sonnet-latest, opus-latest

Mapping (current):
- haiku → claude-3-5-haiku-20241022
- sonnet → claude-sonnet-4-5-20250929
- opus → claude-3-opus-20240229

Implementation:
- Added MODEL_SHORTCUTS dict in core.py
- Added resolve_model_name() function
- CLI resolves shortcuts before API calls
- Full model IDs still accepted
- Changed defaults from full IDs to 'sonnet' shortcut

Benefits:
- Easier to use (just -m haiku instead of long ID)
- Predictable (canonical mappings documented)
- Flexible (can use -latest for newest versions)
- Scripts/docs stay current with shortnames

Opus warning:
- Any opus model triggers cost warning
- User must confirm (y/N) before proceeding
- Prevents accidental expensive API calls
- Warning notes opus may not add value for summaries/transcripts

Testing: Confirmed haiku shortcut resolves correctly, processes successfully.
